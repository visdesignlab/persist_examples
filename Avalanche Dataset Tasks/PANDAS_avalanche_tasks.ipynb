{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522fcbb3-702c-4ea0-91d9-f3b0e71690fc",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰àᦴ΀࣪ʐ怶䦠挠᱊ࠥ䁁䂖㚥煤晬փ⊪ĸ暂䙠঳ैkᑄᄈ妐ଷᓋ⃩䖻Ṯ溜Φ䐡傠Ǒ団Қ′䈤7ࡣᠠ湐戎Ⴆ寐爤䀥屨攠ʕ㥀ǔÚ䍃㏙䤥ࢂ┩勸惀Ēいཆ༠೥∁∀秸㲅䌠ⶬᛦ䙂ǃɡ殍戰泤ᒁ嬦怭㋉匥穃ᄠ彣჻⴦䉫ぢ睤䄁ⵐۻG䀡曑砑䂱竖ᄂ㜿Ꭽ˹ȩᚱ凁兘㑐Ṣ༩Ẻ䳥⑆憰棨忞⍧攈堽Ī僰὆揖䦲ᡊ⛓౱নఱ姼ᤦΧ您別൧儸晋Ǣ熘繀䉁硨K塊়ṹᜰ倳䪂ⵦ⚈ᒽ᪺ᆬ䀠 "
   },
   "source": [
    "# Analyzing Utah Avalanche Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac0d48-2820-4532-bfd2-adf6533d7a2d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to our data analysis study. For this part of the study, you'll be working with a dataset sourced from the [Utah Avalanche Center](https://utahavalanchecenter.org/). The data provides insights into [avalanche occurrences](https://utahavalanchecenter.org/avalanches) in Utah."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905153f6-39fd-4fa8-8eae-fa4818de0df6",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c85d1-1882-4b59-9cc3-26dc6b1e358d",
   "metadata": {},
   "source": [
    "- Pandas is set up and ready for use, along with other Python libraries such as Matplotlib, Seaborn, and Altair for data visualization.\n",
    "- You are allowed to use internet resources like documentation and forums, including Stack Overflow, to assist you in completing the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5d524-883a-460b-8023-2c6aab29570f",
   "metadata": {},
   "source": [
    "## Tasks Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebadbe2-4497-401c-9220-017809e835de",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ૠବɐච䎍倶䊪ᠠ父嬡ᳺ෪ੲࠬ晴ᘦ㍲差Ⱗㅅ)㊤䈽ାƂ刴喋橖㎁混䩟ႠᨰV岸㈔␢⡠䀢焧匠෦భ戰௷㙠̸x䠊䀡᲼䠠洠⺾彡抄䀠㈉㶣㡶ᡈ\\䰩(懄燃㌂㰢ᒌ娾䀶挥㗦ࢢ烡ၕ⬱жㄢ៉䓐͸⮑ŝㄐ䁏你㋲既祸ၛᅀ㢆䰽⫠਀⻚粀幝ඨ࣓叾殪㙡ƨ◓ᰢ寘奇Ⴌ䦦糂礒৉ᱥ䩃༥ס扦᤭ԓ噻ҩᆠ剋࣡ସ䂡▥撰じ㸨䅃䢴മ䂁ᣌ㬫䈁碢➫怢崢悩欂䥑央Ⴧ掴≺䩬槔ဨ搠 "
   },
   "source": [
    "In this study, you are presented with three fundamental data analysis tasks. Each task is designed to test different aspects of data analysis and manipulation.\n",
    "\n",
    "- Carefully follow the step-by-step instructions provided for each task.\n",
    "- As you work through the tasks, take note of any interesting findings or challenges you encounter.\n",
    "- Feel free to add new code and markdown cells in the notebook as necessary to complete the tasks.\n",
    "- Document your findings and any challenges faced during the analysis in markdown cells. This can include observations about the data, any issues encountered, and your overall experience with the task/method.\n",
    "\n",
    "**Support**\n",
    "- If you require assistance or need further clarification on any of the tasks, please let us know.\n",
    "- If you find yourself stuck on a task and feel that you will not make any progress, you have the option to skip the task.\n",
    "- For tasks that build upon the outputs of previous tasks, skipping a task might affect your ability to proceed. If you choose to skip a task, we can assist you by providing the necessary dataset or outputs required for the consecutive tasks. To continue to next tasks:\n",
    "    - Use `h.load_task_output(task_id, subtask_id)` to load necessary data.\n",
    "    - Example: If you skip `2a`, before starting `2b`, run\n",
    "      ```python\n",
    "      \n",
    "      df_task_2a = h.load_task_output(2, 'a')\n",
    "\n",
    "      \n",
    "      ```\n",
    "    - This ensures `df_task_2a` contains the expected output for task `2b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337796b-21e8-4c8c-bbbd-3076a775d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers as h\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "\n",
    "import persist_ext as PR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81aecd-eebf-4624-b9e2-3a864c474c1d",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰Ҁ̡䘠烠嘢త〫‥侴ᓬᛞ拐ৡ䚂ద抭婈ހڅྯ䌲⠠䭈⑁槑焳Ⴥ⩦仃㍶๽㰥Ҩǁ#㟢䌰橠ኤРᜰ͐üメ湁˛㾄ָ̠憴)挄䀦栢琭ٮ㍄ᨡ䌤ᾣㆢ䀣擠䢤㘦Sᥧᡠ戂怡ᠬÖᠻ㹈䐿ة׉妨⃓ࢱ̴㖠᫲徤ฏѤƜڣ回㸦ۡ৵ण䃔戊㘠传ϊ緯挡䘌污䕽庮粓㈨ɪⷠ搃猄ᦨ洘㳌ᤪ̪౦繶ሱ澂䘣⁑Ҝഭ炍孉ृÅ⭱䤾ᢠ䆋෭ኀ㠢Ү≱呺Ấ惑汮ᮯ戩䊏̠˙Ɂ⧽憳㈒ṉⅱਊ⥋⃆瑀⣨  "
   },
   "source": [
    "## Data Description\n",
    "\n",
    "The table below describes the different columns in the dataset. Each row in the dataset represents a reported avalanche with details on location, trigger, and aspect. The data spans multiple years, starting from 2004 up to 2023.\n",
    "\n",
    "| Column          | Description                                                    |\n",
    "|-----------------|----------------------------------------------------------------|\n",
    "| Date            | Date on which the avalanche was recorded                       |\n",
    "| Region          | Region in Utah where the avalanche occurred                    |\n",
    "| Place           | Exact location where the avalanche was recorded                |\n",
    "| Trigger         | Cause of the avalanche                                         |\n",
    "| Weak Layer      | Layer of snow that was weakest and likely to fail              |\n",
    "| Depth_inches    | Depth of the avalanche in inches                               |\n",
    "| Width_inches    | Width of the avalanche in inches                               |\n",
    "| Vertical_inches | Vertical distance covered by the avalanche in inches           |\n",
    "| Aspect          | Direction of the slope where the avalanche occurred            |\n",
    "| Elevation_feet  | Elevation of the location in feet                              |\n",
    "| Coordinates     | Approximate geographical coordinates of the avalanche location |\n",
    "| Comments 1      | Additional comments provided by the reporter                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e426c-89a0-4813-8994-65ebba3f3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./avalanches_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc10d2b-9b1e-46f4-9769-b9a0ac0c9e8f",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ૠ㌬Ā჻R〶䁃lyٹ⁓f䍐ۢ䧪㈉◔ฅޡ䨲⠠䬱䑁棒瀱Ⴭ⭱妻ݎẜʤ䐡傠ǖ悃РኤРᜰ結üメ浪巟㈠ᣠˬ⃪$熒‣㐡㬚͇⦲༿⸂໡壡‡犀⒂ଠᎭॣ䥡⊃㡹稠嬸ූᑤඦҡ哹䑀妨䤝͕䀺慽䈩瞤∡㼚ƶ嵖᯦悡檩ǣ᪀畖o#䖊⠃ܑ璌⃥⹾䞌᧒Т⺈ᆡ⇳㴑䒤㒌ŭ᯸丈◡戱⃰潅Ԧ秐噑๦磜疶溂⒀ᒪ扖⋦ူⶃ㧄మྡྷშ⨭རㅸ瘭ǅ爘䅐䘆砠址䑂㥨㈼㹯䕈䓵ᒹ憴婊③ᤠ "
   },
   "source": [
    "# Task 1: Refining Columns and Preparing Data\n",
    "\n",
    "In the first task we will perform some basic data cleaning operations to get our dataset ready for further tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934b219-dd97-4e3e-9aa9-0dd7a9d62ad5",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ૠ㬠氤〥䌸瀶䀹䁣๲ৠܦᔨҀᣴ䂼䋃‣ᓆ做►䀳ല⠠䭈⑁槑瀱ᝉ⍮冫㭮ẜʦ䒠ǁ#㕀挸晠ኤРᝣၐüムṁǟ㤉䀥䉸攠ʓ㥀ǔÚ⎃஥䤫䒄攭ᇸ惀Ēいᘦᖠೣ䗫⃂変ⱦ{ఽ咼∪⌤䋵汄ჹ䑈冣ᛠඉⵆܔ⍂Þ⍡歹ম悩溩Ǡ媁疖o#䖓瀃ʅ璌⁅溮杲◒Ъ⍁䔶户ࣉⓢ削౐䘰䵥䌂んḸئԢ灐糕൤㧶ⶬ䲱䅲ᗈ撯ౠ惍䈅䥐属ѣ၈⩍སヸ๗௧焤ᄸ䒰Ž䅰䔉燩壥༲刳樲⭮✈咰ᒄ  "
   },
   "source": [
    "### **Task 1a: Remove Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b531d4-2e3a-4e9a-b92a-39aebfc3deb6",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ૠ㬠氤〥䌸瀶䀹䁣๲ৠܦᔨҀᣴ䂼䋃‣ᓆ做►䀳ല⠠䭈⑁槑瀱ᝉ⍮冫㭮ẜʦ䒠ǁ#㕀挸晠ኤРᝣၐüムṁǟ㤉䀥䉸攠ʓ㥀ǔÚ⎃஥䤫䒄攭ᇸ惀Ēいᘦᖠೣ䗫⃂変ⱦ{ఽ咼∪⌤䋵汄ჹ䑈冣ᛠඉⵆܔ⍂Þ⍡歹ম悩溩Ǡ媁疖o#䖓瀃ʅ璌⁅溮杲◒Ъ⍁䔶户ࣉⓢ削౐䘰䵥䌂んḸئԢ灐糕൤㧶ⶬ䲱䅲ᗈ撯ౠ惍䈅䥐属ѣ၈⩍སヸ๗௧焤ᄸ䒰Ž䅰䔉燩壥༲刳樲⭮✈咰ᒄ  "
   },
   "source": [
    "#### **Objective**\n",
    "Remove certain columns to streamline the dataset for further analysis.\n",
    "- **_Comments 1:_** Contains textual comments not crucial for quantitative analysis.\n",
    "- **_Coordinates:_** Detailed location data not needed for the current scope of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a5a56-a415-44d4-9be4-4c4e22f68502",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Column Removal:**\n",
    "\t- Remove the specified columns using Pandas commands.\n",
    "2. **Generate dataframe:**\n",
    "\t- Assign the modified dataframe to variable `df_task_1a`\n",
    "3. **Show Output:**\n",
    "\t- Print the head of `df_task_1a` to show the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15449b-4ead-4e30-b779-dd88a7169454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c62d50-9c88-4ed9-8c97-e552a6e4c620",
   "metadata": {},
   "source": [
    "### **Task 1b: Fix Column Names**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f8f24-afd2-424f-ab34-3e3cde723aa3",
   "metadata": {
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ࢀㆪこ<ǐʀ୥崡汤Ɓ䔠⍂⠨撂䂼ì债晶堪慯掩ᐠ▴ሰ璹㤨䮲减ⓥ䢧᜾㺣䉠ð䀡壅冨㕀ॢȠ௞䠸ᢀ眰䅽忖䀢晐樠Ԃ牠ΈƔڦ❓ሠƠ暬ॡ声Ȅ恨䙜⬮ಬ俨⫣䞐డ㙐᬴⣔䰲ճ㌰䊆၂۔欠㖹┰᧶༨̘ඛฌ琻ɋ㉄ܠ榡哸Ŝ.ؗ⾬ള勑̵㯺ᱱ曨ဩᒡᾪ३⁂惡৒ј⪖ᩋࢶ䥣䍢ᇃ⢀☾ԣ䖎㑝䳓ⳄⓀ᪱ĦҐ戤׌㝔䆦成扉ڠ凈㨣ೣㅘ湏Bࡤ䒠ଈତޗ٩䟨㣜㥨◈ᩲ㭡⟢䀠 "
   },
   "source": [
    "#### **Objective**\n",
    "For this subtask, we will focus on fixing column names to ensure consistency and clarity. We'll start by identifying the issues with the column names, specifically targeting those with a `;` prefix that needs removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ded8ee-7828-494d-afeb-8f5b5b5595f1",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰à⬠氤瀹䂃¦¸˰ҁ①าÐɐ໦䓊禳Ӧ攆⩧琫樁拄䨠ዪन㪌糥效桖㖎槻域℁兀†沤挹䠤傡%梄ؠᮬᢛ䑁圌ḀÐ乒䀡ᵼ䠠洠⺱壹溄䅢ᨀ䈩㙐⠠㲸ब佄⍦儂ト桦{్䂼∠⌤䉶汄ၹ䑨凝۠ඊ⡂ڗ䉂Þ⍾捠ೠ䞫づ㕤䄀൐櫻G䀡擑琑䈂㫶ဲ㜧Ꭱ˹ȥᆲ冲椒咃ᨼᐱᒶ䭢䇡Ᏹユ僌䘼ᚦᩐ挬ദ䌜๵䎃ᅄ〴䦈᥃䌰ࡃ㏙㉬ᜨ⊰ᒊ຤惐氮ண成ɁѦቄwၤቘ㲒㙑⏤咤窤䫳䧚ᕄй  "
   },
   "source": [
    "#### **Instructions**\n",
    "1. **Rename Columns:**\n",
    "    - Employ Pandas commands to rename the columns, eliminating the leading \";\" as specified:\n",
    "        - _;Aspect_ → _Aspect_\n",
    "        - _;Region_ → _Region_\n",
    "        - _;Trigger_ → _Trigger_\n",
    "        - _;Weak Layer_ → _Weak Layer_\n",
    "2. **Generate dataframe:**\n",
    "    - Assign the updated dataframe to variable `df_task_1b`.\n",
    "3. **Show Output:**\n",
    "    - Print the head of `df_task_1b` to confirm the updated column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660a9a4-bbf0-42d9-aacf-ae4c6afa5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_1a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875e7fc-ec40-4241-aa94-87b31e59944f",
   "metadata": {},
   "source": [
    "## **Task 1c: Correcting Data Type of 'Depth_inches'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7eebda-1b60-4bff-89d3-a5548a601b49",
   "metadata": {},
   "source": [
    "#### **Objective**\n",
    "In this task, we will address a data type issue in the `Depth_inches` column of our dataframe. This column is incorrectly formatted as a object (string) due to the presence of the inches symbol `\"`.\n",
    "\n",
    "Remove any inches symbols `\"` from the `Depth_inches` column and convert it to a float data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bff61-9a8d-4114-97f3-4f6d0cd0e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_1b.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843f33c-83c6-4d44-a986-820ff11585dc",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ೠ↺΀Ơ瘢ᠢᠫ‵䁇ፉ䐢怣侽振ၑ䶃᪰঵䂆⹉多䪲┠অҤᴶṂ㊵嘊᭓ㄅ毠㒰䣑Tဠ㚉҃೨ɰ䂠̂҆;䘸Ϥၗ眼䋐Ð䬹†于␠㚠ᝨ沂烒⎁൸憈㬸ᐠṬү䓧Ⴠ椓ᣘ⌚ᆠᛦކ䜱ǁ䅀㖖नຂᩈ桫〦塵偢㶱⢠⾱䂅埦䖃塀㪂ₐ暸ᵽ䀳怠煼砤慸㴻ࠩ㯏姭ঌ䄢䣫䧙㱈刱ㄿᬦ䩫⏃㌱楫磤ⰴຣα晼ਫ䎁㺕䮃Ⴤご䖂᥂挰ы゙㈬ܠ₰ᑊІ僑汮ޯ戩䉱៪怡⹁ᡤ甑擸抯॰ᡬᅓ╄璚╄㈠ "
   },
   "source": [
    "#### **Instructions**\n",
    "\n",
    "1. **Remove Inches Symbol and Correct Format:**\n",
    "    - Use Pandas to replace the inches symbol in the `Depth_inches` column.\n",
    "2. **Convert Data Type:**\n",
    "    - Convert the `Depth_inches` column to float.\n",
    "3. **Generate Dataframe:**\n",
    "    - Save the updated dataframe as `df_task_1c`.\n",
    "4. **Show Output:**\n",
    "    - Print the dtypes of `df_task_1c` to confirm the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420d86a-7cff-49d8-a92f-6c6fda61e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_1b.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ed0a8-dc0b-4ac8-8afd-b554ea8f4d28",
   "metadata": {},
   "source": [
    "# Task 2: Filtering data\n",
    "\n",
    "In Task 2, we further improve our data by removing outliers and removing certain records to have more consistent data. \n",
    "\n",
    "We will also take a brief look at relations between cause of an avalanche (`Trigger`) and failure point of ice (`Weak Layer`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25d8f5-98e8-4b0f-9358-ed85f77cbed8",
   "metadata": {},
   "source": [
    "## **Task 2a: Remove Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53a605-2d10-4457-9625-70ce88f04f3b",
   "metadata": {},
   "source": [
    "#### **Objective**\n",
    "In this task, we address data accuracy by filtering out anomalies in elevation data. We observe some records with elevations outside the plausible range for Utah, suggesting recording errors.\n",
    "\n",
    "Remove avalanche records with elevations below 2000 feet and above 13500 feet, which are outside the realistic range for Utah."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3529a-641b-42b1-9e4d-d51461be760f",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Locate Anomalous Data:**\n",
    "    - Refer to the _seaborn_ scatterplot for `Elevation_feet` vs `Vertical_inches`\n",
    "    - Write code to identify records where `Elevation_feet` is either below 2100 feet or above 13500 feet.\n",
    "3. **Remove Anomalies:**\n",
    "    - Use Pandas commands to filter out these anomalous records from the dataframe.\n",
    "4. **Generate Dataframe:**\n",
    "    - Save the cleaned dataframe as `df_task_2a`.\n",
    "5. **Plot Output:**\n",
    "    - Recreate the scatterplot from step 1 in a new cell using `df_task_2a`.\n",
    "    - Print the head of `df_task_2a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74ffa1-bc7b-49b0-af70-8db34385000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2a = df_task_1c\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.scatterplot(data=df_task_2a, x='Elevation_feet', y='Vertical_inches')\n",
    "\n",
    "plt.xlabel('Elevation (feet)')\n",
    "plt.ylabel('Vertical Distance (inches)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447b862-90c6-4ed7-b48b-baa1cf74ac2d",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰࣠✴ƀචآ൐ୠᣠ⭎¸䚀ᶥ<⋵攴≲䦆ᨐ҅䁲䥈kᑄᄈ夒ર囇⣹嗛➎帣ᢤ䀡傠Ǘ熠㥀ॢȠா䠸ᢀ㜰䁾巒•橀樠Ꮃ䤠ව׷核ᱬ䣘紲₞૦Ԡ޳ŀ凐栠晬╍ಲ䖽䂇栢沀團ᄰ㸸ሪ喊Ⴇ♁䐔粶ᦞΎ刺ࠥ簉窺簲ᖢොሧ҉䃵ⰡḠޅ毗䘤䗾墥୚渍磆搰㣄䆯䌭䊣ᎄ䢩妈䪑䴹ҩ䋫◂რ⢒ઉ⃁⽌囦媍ኘ垤䢠䍤⒂Йᘷ੡䄀҂Ქ⃱琿啡憐㲞ၯ䌨焠ᗿ搫໎㒫ல熪Ņ⎃ऺᥛ⃄ᅠ  "
   },
   "source": [
    "## **Task 2b: Filtering Out Old Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b30d4-4762-4bc0-bfd3-6f1486b55ad6",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰࣠✴ƀචآ൐ୠᣠ⭎¸䚀ᶥ<⋵攴≲䦆ᨐ҅䁲䥈kᑄᄈ夒ર囇⣹嗛➎帣ᢤ䀡傠Ǘ熠㥀ॢȠா䠸ᢀ㜰䁾巒•橀樠Ꮃ䤠ව׷核ᱬ䣘紲₞૦Ԡ޳ŀ凐栠晬╍ಲ䖽䂇栢沀團ᄰ㸸ሪ喊Ⴇ♁䐔粶ᦞΎ刺ࠥ簉窺簲ᖢොሧ҉䃵ⰡḠޅ毗䘤䗾墥୚渍磆搰㣄䆯䌭䊣ᎄ䢩妈䪑䴹ҩ䋫◂რ⢒ઉ⃁⽌囦媍ኘ垤䢠䍤⒂Йᘷ੡䄀҂Ქ⃱琿啡憐㲞ၯ䌨焠ᗿ搫໎㒫ல熪Ņ⎃ऺᥛ⃄ᅠ  "
   },
   "source": [
    "The interactive barchart below, shows the data aggregated by year. There are noticeably fewer records for the years before 2010.\n",
    "\n",
    "During this subtask we will remove the older records, keeping only the records post 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb76fc-6ea4-4296-8c50-39381f09d9c8",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Create Bar Chart with Seaborn:**\n",
    "    - Use the Seaborn plot with bar chart visualizing the number of avalanches per year. \n",
    "2. **Identify Sparse Years:**\n",
    "    - Based on the bar chart, identify years before 2010 with fewer avalanche records.\n",
    "3. **Code to Filter Out Sparse Years:**\n",
    "    - Write Pandas code to exclude these years from the dataset.\n",
    "4. **Show Output:**\n",
    "    - Print the head of `df_task_2b` and optionally recreate the bar chart to show the dataset focusing on years 2010 and onwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56985931-4907-4683-ad34-fadad6b09568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2b = df_task_2a\n",
    "df_task_2b = df_task_2b.convert_dtypes()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=df_task_2b[\"Date\"].dt.year)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('# of records')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fdc8b-168e-40e2-861c-d23095a4a8d8",
   "metadata": {},
   "source": [
    "## **Task 2c: Identifying frequently failing `Weak Layers` for avalanches triggered by _'snowboarders'_ and _'skiers'_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda516c-ca0c-4ecc-bd48-e40287c4193d",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Identify Predominant Weak Layer:**\n",
    "    - Determine the most common weak layer for these `Snowboarder` and `Skier` triggers for the data.\n",
    "2. **Output:**\n",
    "    - Note in markdown cell both the name of the layer and frequency for each of the above trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7aebd-b709-4b35-80c3-a7c872de6b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "052071fd-2755-4c50-b65b-d4ade860d910",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92b8a9-c3a8-4a57-be3b-c9cdc977092c",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ೠ⬠沈֭⠸恍ff9昽䁮ۦ࠰汬നڀপ㈏䣹厠㬦ተ¶⡨⇱㌀ᅁ╶庳ୖ⹜簧ଭࠣ℠Έ橦औ䁅ШNႦ〡岁䏼ℭ㞁摞+㢱䨠Ԋ牠Έƕ٦枓቙ჿਸ旑䅠Ȅ恨戬പᎭె䝁净儬Ö᡻⌸䐱䘩׍宨⇓ࡱ䍲ච᫲壬จ䙤Ɲڣ圖⍞ᛡ৽ण䃔戋㘠传ϊ揿振䘌汣䓼㹮䘫㈨ቺ⩯↧ᄨXഉ⠅ㅖࢢ侸⇄僡ಖࡢҁ氣С宍劄倶䢡⍤㲂̤㚗٩䄀硂಩⍰琦ᦪ悑屾ᾨႨ㺜*瘪䋇޶⋤ᓘ犠䭳঺᥇⏰ᅠ  "
   },
   "source": [
    "## Task 3: Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da80393-7f4b-4115-a4d1-5b7978e11358",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰â䂬ƀᶷ琶䁋c<搔怳ଚ䠳亡䈤㉕⁖ì⺇⊨攩ᐠ▨戰㑹㧒妰囇⃹喧㝞Ţ∠桠ûு䁔䈠ॢȠந㺸ᢀ盅⻿夣•晀樠Ԃ牠Έƕ稦❳ሪὦ䰼⇑䅠Ȅ恨㙎⌯᪴䶹䈁ℓ琡㙐ᮂ⢨᢬थ⧓ࡢ㌰ሚ璋U䳘䠵欯Т縔絭Sㄭۡ۹ण䉔悊㘠传ϒ橷挣⌏ⱡ䕼庮打㈨ɼ榨慢䃂॑瑐繦┭ڼ㾬⎂厃焸牘榥ᄨ䦯ඦ嘔㗆溄⒀擒摖≦၀ⶃ㧄మޢᄴ䩍རへ∦䯧焤愷䍠Žƀ夅შ㤙༾′⪂㵩掳婊⢄  "
   },
   "source": [
    "### Task 3a: Creating and assigning 'Avalanche Season'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402412ae-3b84-4fec-9c01-94dd203a7b1f",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰â䂬ƀᶷ琶䁋c<搔怳ଚ䠳亡䈤㉕⁖ì⺇⊨攩ᐠ▨戰㑹㧒妰囇⃹喧㝞Ţ∠桠ûு䁔䈠ॢȠந㺸ᢀ盅⻿夣•晀樠Ԃ牠Έƕ稦❳ሪὦ䰼⇑䅠Ȅ恨㙎⌯᪴䶹䈁ℓ琡㙐ᮂ⢨᢬थ⧓ࡢ㌰ሚ璋U䳘䠵欯Т縔絭Sㄭۡ۹ण䉔悊㘠传ϒ橷挣⌏ⱡ䕼庮打㈨ɼ榨慢䃂॑瑐繦┭ڼ㾬⎂厃焸牘榥ᄨ䦯ඦ嘔㗆溄⒀擒摖≦၀ⶃ㧄మޢᄴ䩍རへ∦䯧焤愷䍠Žƀ夅შ㤙༾′⪂㵩掳婊⢄  "
   },
   "source": [
    "#### **Objective**\n",
    "\n",
    "In this subtask, we'll introduce a new categorical variable named `Avalanche Season` into our dataset. This addition aims to classify each avalanche record into different parts of the avalanche season (Start, Middle, End) based on the month it occurred.\n",
    "\n",
    "Create a new category `Avalanche Season` in the dataset and assign each record to `Start`, `Middle`, or `End` of the avalanche season based on its month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48aaa8-7017-43de-8bd8-190ed9fe4dbd",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Create New Variable:**\n",
    "    - Add a new column `Avalanche Season` to the DataFrame.\n",
    "2. **Assign Category:**\n",
    "    - Using the `month` from the `Date` column assign proper values to the new category.\n",
    "    - You should use the following ranges for assigning proper categories:\n",
    "        - `Start` of Season: October, November, December, January, February\n",
    "    \t- `Middle` of Season: March, April, May,\n",
    "    \t- `End` of Season: June, July, August, September\n",
    "3. **Generate Dataframe:**\n",
    "    - Save the modified DataFrame with the new `Avalanche Season` category to `df_task_3a`.\n",
    "4. **Show Output:**\n",
    "    - Display the head of `df_task_3a` to confirm the addition and categorization of the new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb77fc-69d5-4bfc-beff-ae54ad5c19bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e82a42-5f2f-44e1-b9a5-2e2d403b2ff3",
   "metadata": {},
   "source": [
    "# **Task 3b:## **Analyzing Top Avalanche Trigger by Season**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40bad3-436e-426c-a02f-f205597e90b5",
   "metadata": {},
   "source": [
    "#### **Objective**\n",
    "In this subtask, we'll analyze which trigger is most prevalent for avalanches in each season phase (Start, Middle, End) using the `Avalanche Season` category created in Task 3a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de570e5-ab49-4e8f-b2b1-86b9a2701ece",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Analyze Triggers by Season:*\n",
    "\t- Determine the most common trigger for each season.\n",
    "2. **Present Findings:**\n",
    "\t- Note in markdown cell both the name and frequency for each trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967e8a0-84b9-4723-ab6f-c1660c7a46fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d51a13e2-c66f-45e6-ba89-2ad0c69afa58",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  }
 ],
 "metadata": {
  "__CATEGORIES_META__": "{\"categories\":{\"Avlanche Season Phase\":{\"name\":\"Avlanche Season Phase\",\"options\":{\"_None\":{\"name\":\"None\"},\"Start\":{\"name\":\"Start\"},\"Middle\":{\"name\":\"Middle\"},\"End\":{\"name\":\"End\"}}}},\"activeCategoryName\":\"Avlanche Season Phase\"}",
  "__persist_keys_record": [
   "__GENERATED_DATAFRAMES__",
   "__persist_nb_uuid__",
   "trrack_graph",
   "show_aggregate_origin"
  ],
  "__persist_nb_uuid__": "96fc8666-4608-44be-a80e-3acb15011cee",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
