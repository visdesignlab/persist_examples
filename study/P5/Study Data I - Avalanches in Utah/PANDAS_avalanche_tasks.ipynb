{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522fcbb3-702c-4ea0-91d9-f3b0e71690fc",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰àᦴ΀࣪ʐ怶䦠挠᱊ࠥ䁁䂖㚥煤晬փ⊪ĸ暂䙠঳ैkᑄᄈ妐ଷᓋ⃩䖻Ṯ溜Φ䐡傠Ǒ団Қ′䈤7ࡣᠠ湐戎Ⴆ寐爤䀥屨攠ʕ㥀ǔÚ䍃㏙䤥ࢂ┩勸惀Ēいཆ༠೥∁∀秸㲅䌠ⶬᛦ䙂ǃɡ殍戰泤ᒁ嬦怭㋉匥穃ᄠ彣჻⴦䉫ぢ睤䄁ⵐۻG䀡曑砑䂱竖ᄂ㜿Ꭽ˹ȩᚱ凁兘㑐Ṣ༩Ẻ䳥⑆憰棨忞⍧攈堽Ī僰὆揖䦲ᡊ⛓౱নఱ姼ᤦΧ您別൧儸晋Ǣ熘繀䉁硨K塊়ṹᜰ倳䪂ⵦ⚈ᒽ᪺ᆬ䀠 "
   },
   "source": [
    "# Analyzing Utah Avalanche Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b6afb-1443-4d42-b48d-b1101ef79cd0",
   "metadata": {},
   "source": [
    "**Participant ID:**\n",
    "\n",
    "**Date / Time:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac0d48-2820-4532-bfd2-adf6533d7a2d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to our data analysis study. For this part of the study, you'll be working with a dataset sourced from the [Utah Avalanche Center](https://utahavalanchecenter.org/). The data provides insights into [avalanche occurrences](https://utahavalanchecenter.org/avalanches) in Utah."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c85d1-1882-4b59-9cc3-26dc6b1e358d",
   "metadata": {},
   "source": [
    "- You will use pandas to complete **data cleanup and manipulation** tasks. \n",
    "- Carefully follow the step-by-step instructions provided for each task.\n",
    "- Pandas is set up and ready for use, along with other Python libraries such as Matplotlib, Seaborn, and Altair for data visualization.\n",
    "- You are allowed to use internet resources like documentation and forums, including Stack Overflow, to assist you in completing the tasks.\n",
    "- In some cases, you will be asked to document your findings. Please do this in writing in a markdown cell.\n",
    "- As you work through the tasks, take note of any interesting findings or challenges with the software or pandas that you may encounter, either by speaking your thoughts out loud or taking notes in a markdown cell.\n",
    "- Feel free to add new code and markdown cells in the notebook as necessary to complete the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337796b-21e8-4c8c-bbbd-3076a775d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers as h\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81aecd-eebf-4624-b9e2-3a864c474c1d",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰Ҁ̡䘠烠嘢త〫‥侴ᓬᛞ拐ৡ䚂ద抭婈ހڅྯ䌲⠠䭈⑁槑焳Ⴥ⩦仃㍶๽㰥Ҩǁ#㟢䌰橠ኤРᜰ͐üメ湁˛㾄ָ̠憴)挄䀦栢琭ٮ㍄ᨡ䌤ᾣㆢ䀣擠䢤㘦Sᥧᡠ戂怡ᠬÖᠻ㹈䐿ة׉妨⃓ࢱ̴㖠᫲徤ฏѤƜڣ回㸦ۡ৵ण䃔戊㘠传ϊ緯挡䘌污䕽庮粓㈨ɪⷠ搃猄ᦨ洘㳌ᤪ̪౦繶ሱ澂䘣⁑Ҝഭ炍孉ृÅ⭱䤾ᢠ䆋෭ኀ㠢Ү≱呺Ấ惑汮ᮯ戩䊏̠˙Ɂ⧽憳㈒ṉⅱਊ⥋⃆瑀⣨  "
   },
   "source": [
    "## Data Description\n",
    "\n",
    "The table below describes the different columns in the dataset. Each row in the dataset represents a reported avalanche with details on location, trigger, and aspect. The data spans multiple years, starting from 2004 up to 2023.\n",
    "\n",
    "| Column          | Description                                                    |\n",
    "|-----------------|----------------------------------------------------------------|\n",
    "| Region          | Region in Utah where the avalanche occurred                    |\n",
    "| Month           | Month in which the avalanche was recorded                      |\n",
    "| Day             | Day on which the avalanche was recorded                        |\n",
    "| Year            | Year in which the avalanche was recorded                       |\n",
    "| Trigger         | Cause of the avalanche                                         |\n",
    "| Weak Layer      | Layer of snow that was weakest and likely to fail              |\n",
    "| Depth_inches    | Depth of the avalanche in inches                               |\n",
    "| Vertical_inches | Vertical distance covered by the avalanche in inches           |\n",
    "| Aspect          | Direction of the slope where the avalanche occurred            |\n",
    "| Elevation_feet  | Elevation of the location in feet                              |\n",
    "| Coordinates     | Approximate geographical coordinates of the avalanche location |\n",
    "| Comments 1      | Additional comments provided by the reporter                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e426c-89a0-4813-8994-65ebba3f3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./avalanches_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc10d2b-9b1e-46f4-9769-b9a0ac0c9e8f",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ૠ㌬Ā჻R〶䁃lyٹ⁓f䍐ۢ䧪㈉◔ฅޡ䨲⠠䬱䑁棒瀱Ⴭ⭱妻ݎẜʤ䐡傠ǖ悃РኤРᜰ結üメ浪巟㈠ᣠˬ⃪$熒‣㐡㬚͇⦲༿⸂໡壡‡犀⒂ଠᎭॣ䥡⊃㡹稠嬸ූᑤඦҡ哹䑀妨䤝͕䀺慽䈩瞤∡㼚ƶ嵖᯦悡檩ǣ᪀畖o#䖊⠃ܑ璌⃥⹾䞌᧒Т⺈ᆡ⇳㴑䒤㒌ŭ᯸丈◡戱⃰潅Ԧ秐噑๦磜疶溂⒀ᒪ扖⋦ူⶃ㧄మྡྷშ⨭རㅸ瘭ǅ爘䅐䘆砠址䑂㥨㈼㹯䕈䓵ᒹ憴婊③ᤠ "
   },
   "source": [
    "# Task 1: Column Names and Data Types\n",
    "\n",
    "In the first task we will perform some basic data cleaning operations to get our dataset ready for further tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934b219-dd97-4e3e-9aa9-0dd7a9d62ad5",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ૠ㬠氤〥䌸瀶䀹䁣๲ৠܦᔨҀᣴ䂼䋃‣ᓆ做►䀳ല⠠䭈⑁槑瀱ᝉ⍮冫㭮ẜʦ䒠ǁ#㕀挸晠ኤРᝣၐüムṁǟ㤉䀥䉸攠ʓ㥀ǔÚ⎃஥䤫䒄攭ᇸ惀Ēいᘦᖠೣ䗫⃂変ⱦ{ఽ咼∪⌤䋵汄ჹ䑈冣ᛠඉⵆܔ⍂Þ⍡歹ম悩溩Ǡ媁疖o#䖓瀃ʅ璌⁅溮杲◒Ъ⍁䔶户ࣉⓢ削౐䘰䵥䌂んḸئԢ灐糕൤㧶ⶬ䲱䅲ᗈ撯ౠ惍䈅䥐属ѣ၈⩍སヸ๗௧焤ᄸ䒰Ž䅰䔉燩壥༲刳樲⭮✈咰ᒄ  "
   },
   "source": [
    "### **Task 1a: Remove Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b531d4-2e3a-4e9a-b92a-39aebfc3deb6",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ૠ㬠氤〥䌸瀶䀹䁣๲ৠܦᔨҀᣴ䂼䋃‣ᓆ做►䀳ല⠠䭈⑁槑瀱ᝉ⍮冫㭮ẜʦ䒠ǁ#㕀挸晠ኤРᝣၐüムṁǟ㤉䀥䉸攠ʓ㥀ǔÚ⎃஥䤫䒄攭ᇸ惀Ēいᘦᖠೣ䗫⃂変ⱦ{ఽ咼∪⌤䋵汄ჹ䑈冣ᛠඉⵆܔ⍂Þ⍡歹ম悩溩Ǡ媁疖o#䖓瀃ʅ璌⁅溮杲◒Ъ⍁䔶户ࣉⓢ削౐䘰䵥䌂んḸئԢ灐糕൤㧶ⶬ䲱䅲ᗈ撯ౠ惍䈅䥐属ѣ၈⩍སヸ๗௧焤ᄸ䒰Ž䅰䔉燩壥༲刳樲⭮✈咰ᒄ  "
   },
   "source": [
    "Remove the follwoing columns to streamline the dataset for further analysis:\n",
    "- **_Comments 1:_** Contains textual comments not crucial for quantitative analysis.\n",
    "- **_Coordinates:_** Detailed location data not needed for the current scope of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a5a56-a415-44d4-9be4-4c4e22f68502",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Column Removal:**\n",
    "\t- Remove the specified columns using Pandas commands.\n",
    "2. **Generate dataframe:**\n",
    "\t- Assign the modified dataframe to variable `df_task_1a`\n",
    "3. **Show Output:**\n",
    "\t- Print the head of `df_task_1a` to show the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15449b-4ead-4e30-b779-dd88a7169454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c62d50-9c88-4ed9-8c97-e552a6e4c620",
   "metadata": {},
   "source": [
    "### **Task 1b: Fix Column Names**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f8f24-afd2-424f-ab34-3e3cde723aa3",
   "metadata": {
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ࢀㆪこ<ǐʀ୥崡汤Ɓ䔠⍂⠨撂䂼ì债晶堪慯掩ᐠ▴ሰ璹㤨䮲减ⓥ䢧᜾㺣䉠ð䀡壅冨㕀ॢȠ௞䠸ᢀ眰䅽忖䀢晐樠Ԃ牠ΈƔڦ❓ሠƠ暬ॡ声Ȅ恨䙜⬮ಬ俨⫣䞐డ㙐᬴⣔䰲ճ㌰䊆၂۔欠㖹┰᧶༨̘ඛฌ琻ɋ㉄ܠ榡哸Ŝ.ؗ⾬ള勑̵㯺ᱱ曨ဩᒡᾪ३⁂惡৒ј⪖ᩋࢶ䥣䍢ᇃ⢀☾ԣ䖎㑝䳓ⳄⓀ᪱ĦҐ戤׌㝔䆦成扉ڠ凈㨣ೣㅘ湏Bࡤ䒠ଈତޗ٩䟨㣜㥨◈ᩲ㭡⟢䀠 "
   },
   "source": [
    "It looks like something went wrong when reading the file and some column headers start with a `;`. **Please remove the semicolon from all headers**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ded8ee-7828-494d-afeb-8f5b5b5595f1",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰à⬠氤瀹䂃¦¸˰ҁ①าÐɐ໦䓊禳Ӧ攆⩧琫樁拄䨠ዪन㪌糥效桖㖎槻域℁兀†沤挹䠤傡%梄ؠᮬᢛ䑁圌ḀÐ乒䀡ᵼ䠠洠⺱壹溄䅢ᨀ䈩㙐⠠㲸ब佄⍦儂ト桦{్䂼∠⌤䉶汄ၹ䑨凝۠ඊ⡂ڗ䉂Þ⍾捠ೠ䞫づ㕤䄀൐櫻G䀡擑琑䈂㫶ဲ㜧Ꭱ˹ȥᆲ冲椒咃ᨼᐱᒶ䭢䇡Ᏹユ僌䘼ᚦᩐ挬ദ䌜๵䎃ᅄ〴䦈᥃䌰ࡃ㏙㉬ᜨ⊰ᒊ຤惐氮ண成ɁѦቄwၤቘ㲒㙑⏤咤窤䫳䧚ᕄй  "
   },
   "source": [
    "#### **Instructions**\n",
    "1. **Rename Columns:**\n",
    "    - Employ Pandas commands to rename the columns, eliminating the leading \";\" as specified:\n",
    "        - _;Aspect_ → _Aspect_\n",
    "        - _;Region_ → _Region_\n",
    "        - _;Trigger_ → _Trigger_\n",
    "        - _;Weak Layer_ → _Weak Layer_\n",
    "2. **Generate dataframe:**\n",
    "    - Assign the updated dataframe to variable `df_task_1b`.\n",
    "3. **Verify the Output:**\n",
    "    - Print the head of `df_task_1b` to confirm the updated column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660a9a4-bbf0-42d9-aacf-ae4c6afa5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_1a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875e7fc-ec40-4241-aa94-87b31e59944f",
   "metadata": {},
   "source": [
    "## **Task 1c: Correcting Data Type of 'Depth_inches'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7eebda-1b60-4bff-89d3-a5548a601b49",
   "metadata": {},
   "source": [
    "There is a data type issue in the `Depth_inches` column of our dataframe. This column is incorrectly formatted as an object (string) due to the presence of the inches symbol `\"`.\n",
    "\n",
    "Remove any inches symbols `\"` from the `Depth_inches` column and convert it to a float data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d4cc0-e6fc-4a65-9014-75111f641c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_1b.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843f33c-83c6-4d44-a986-820ff11585dc",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ೠ↺΀Ơ瘢ᠢᠫ‵䁇ፉ䐢怣侽振ၑ䶃᪰঵䂆⹉多䪲┠অҤᴶṂ㊵嘊᭓ㄅ毠㒰䣑Tဠ㚉҃೨ɰ䂠̂҆;䘸Ϥၗ眼䋐Ð䬹†于␠㚠ᝨ沂烒⎁൸憈㬸ᐠṬү䓧Ⴠ椓ᣘ⌚ᆠᛦކ䜱ǁ䅀㖖नຂᩈ桫〦塵偢㶱⢠⾱䂅埦䖃塀㪂ₐ暸ᵽ䀳怠煼砤慸㴻ࠩ㯏姭ঌ䄢䣫䧙㱈刱ㄿᬦ䩫⏃㌱楫磤ⰴຣα晼ਫ䎁㺕䮃Ⴤご䖂᥂挰ы゙㈬ܠ₰ᑊІ僑汮ޯ戩䉱៪怡⹁ᡤ甑擸抯॰ᡬᅓ╄璚╄㈠ "
   },
   "source": [
    "#### **Instructions**\n",
    "\n",
    "1. **Remove Inches Symbol and Correct Format:**\n",
    "    - Use Pandas to replace the inches symbol in the `Depth_inches` column.\n",
    "2. **Convert Data Type:**\n",
    "    - Convert the `Depth_inches` column to float.\n",
    "3. **Generate Dataframe:**\n",
    "    - Save the updated dataframe as `df_task_1c`.\n",
    "4. **Show Output:**\n",
    "    - Print the dtypes of `df_task_1c` to confirm the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420d86a-7cff-49d8-a92f-6c6fda61e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_1b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ed0a8-dc0b-4ac8-8afd-b554ea8f4d28",
   "metadata": {},
   "source": [
    "# Task 2: Filtering data\n",
    "\n",
    "In Task 2, we further improve our data by removing outliers and removing certain records to have more consistent data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25d8f5-98e8-4b0f-9358-ed85f77cbed8",
   "metadata": {},
   "source": [
    "## **Task 2a: Remove Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53a605-2d10-4457-9625-70ce88f04f3b",
   "metadata": {},
   "source": [
    "In this task, we address data accuracy by filtering out anomalies in the elevation data. We observe some records with elevations outside the plausible range for Utah, suggesting recording errors.\n",
    "\n",
    "**Remove avalanche records with elevations below ~4,000 feet and above ~15,000 feet, which are outside the realistic range for Utah.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3529a-641b-42b1-9e4d-d51461be760f",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Identify and Remove Anomalies:**\n",
    "    - Refer to the _seaborn_ scatterplot for `Elevation_feet` vs `Vertical_inches`\n",
    "    - Use Pandas commands to filter out these anomalous records where Elevation_feet is either below ~4,000 feet or above ~15,000 feet, from the dataframe.\n",
    "2. **Generate Dataframe:**\n",
    "    - Save the cleaned dataframe as `df_task_2a`.\n",
    "3. **Plot Output:**\n",
    "    - Recreate the scatterplot from step 1 in a new cell using `df_task_2a`.\n",
    "    - Print the head of `df_task_2a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74ffa1-bc7b-49b0-af70-8db34385000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scatterplot Code Start\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.scatterplot(data=df_task_1c, x='Elevation_feet', y='Vertical_inches')\n",
    "\n",
    "plt.xlabel('Elevation (feet)')\n",
    "plt.ylabel('Vertical Distance (inches)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "df_task_1c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447b862-90c6-4ed7-b48b-baa1cf74ac2d",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰࣠✴ƀචآ൐ୠᣠ⭎¸䚀ᶥ<⋵攴≲䦆ᨐ҅䁲䥈kᑄᄈ夒ર囇⣹嗛➎帣ᢤ䀡傠Ǘ熠㥀ॢȠா䠸ᢀ㜰䁾巒•橀樠Ꮃ䤠ව׷核ᱬ䣘紲₞૦Ԡ޳ŀ凐栠晬╍ಲ䖽䂇栢沀團ᄰ㸸ሪ喊Ⴇ♁䐔粶ᦞΎ刺ࠥ簉窺簲ᖢොሧ҉䃵ⰡḠޅ毗䘤䗾墥୚渍磆搰㣄䆯䌭䊣ᎄ䢩妈䪑䴹ҩ䋫◂რ⢒ઉ⃁⽌囦媍ኘ垤䢠䍤⒂Йᘷ੡䄀҂Ქ⃱琿啡憐㲞ၯ䌨焠ᗿ搫໎㒫ல熪Ņ⎃ऺᥛ⃄ᅠ  "
   },
   "source": [
    "## **Task 2b: Filtering Out Old Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b30d4-4762-4bc0-bfd3-6f1486b55ad6",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰࣠✴ƀචآ൐ୠᣠ⭎¸䚀ᶥ<⋵攴≲䦆ᨐ҅䁲䥈kᑄᄈ夒ર囇⣹嗛➎帣ᢤ䀡傠Ǘ熠㥀ॢȠா䠸ᢀ㜰䁾巒•橀樠Ꮃ䤠ව׷核ᱬ䣘紲₞૦Ԡ޳ŀ凐栠晬╍ಲ䖽䂇栢沀團ᄰ㸸ሪ喊Ⴇ♁䐔粶ᦞΎ刺ࠥ簉窺簲ᖢොሧ҉䃵ⰡḠޅ毗䘤䗾墥୚渍磆搰㣄䆯䌭䊣ᎄ䢩妈䪑䴹ҩ䋫◂რ⢒ઉ⃁⽌囦媍ኘ垤䢠䍤⒂Йᘷ੡䄀҂Ქ⃱琿啡憐㲞ၯ䌨焠ᗿ搫໎㒫ல熪Ņ⎃ऺᥛ⃄ᅠ  "
   },
   "source": [
    "The interactive barchart below, shows the data aggregated by year. There are noticeably fewer records for the years before 2010.\n",
    "\n",
    "During this subtask we will remove the older records, keeping only the records for years 2010 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb76fc-6ea4-4296-8c50-39381f09d9c8",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Identify Sparse Years:**\n",
    "    - Use the Seaborn plot with bar chart visualizing the number of avalanches per year. \n",
    "    - Based on the bar chart, identify years before 2010 with fewer avalanche records.\n",
    "2. **Filter Out Sparse Years:**\n",
    "    - Write Pandas code to exclude these years from the dataset.\n",
    "3. **Show Output:**\n",
    "    - Print the head of `df_task_2b` and recreate the bar chart to show the dataset focusing on years 2010 and onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56985931-4907-4683-ad34-fadad6b09568",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date time\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=df_task_2a[\"Year\"])\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('# of records')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "df_task_2a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92b8a9-c3a8-4a57-be3b-c9cdc977092c",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰ೠ⬠沈֭⠸恍ff9昽䁮ۦ࠰汬നڀপ㈏䣹厠㬦ተ¶⡨⇱㌀ᅁ╶庳ୖ⹜簧ଭࠣ℠Έ橦औ䁅ШNႦ〡岁䏼ℭ㞁摞+㢱䨠Ԋ牠Έƕ٦枓቙ჿਸ旑䅠Ȅ恨戬പᎭె䝁净儬Ö᡻⌸䐱䘩׍宨⇓ࡱ䍲ච᫲壬จ䙤Ɲڣ圖⍞ᛡ৽ण䃔戋㘠传ϊ揿振䘌汣䓼㹮䘫㈨ቺ⩯↧ᄨXഉ⠅ㅖࢢ侸⇄僡ಖࡢҁ氣С宍劄倶䢡⍤㲂̤㚗٩䄀硂಩⍰琦ᦪ悑屾ᾨႨ㺜*瘪䋇޶⋤ᓘ犠䭳঺᥇⏰ᅠ  "
   },
   "source": [
    "## Task 3: Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da80393-7f4b-4115-a4d1-5b7978e11358",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰â䂬ƀᶷ琶䁋c<搔怳ଚ䠳亡䈤㉕⁖ì⺇⊨攩ᐠ▨戰㑹㧒妰囇⃹喧㝞Ţ∠桠ûு䁔䈠ॢȠந㺸ᢀ盅⻿夣•晀樠Ԃ牠Έƕ稦❳ሪὦ䰼⇑䅠Ȅ恨㙎⌯᪴䶹䈁ℓ琡㙐ᮂ⢨᢬थ⧓ࡢ㌰ሚ璋U䳘䠵欯Т縔絭Sㄭۡ۹ण䉔悊㘠传ϒ橷挣⌏ⱡ䕼庮打㈨ɼ榨慢䃂॑瑐繦┭ڼ㾬⎂厃焸牘榥ᄨ䦯ඦ嘔㗆溄⒀擒摖≦၀ⶃ㧄మޢᄴ䩍རへ∦䯧焤愷䍠Žƀ夅შ㤙༾′⪂㵩掳婊⢄  "
   },
   "source": [
    "### **Task 3a: Creating and assigning 'Avalanche Season'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402412ae-3b84-4fec-9c01-94dd203a7b1f",
   "metadata": {
    "__GENERATED_DATAFRAMES__": "{\"nodeDataframes\":{},\"graphDataframes\":null}",
    "show_aggregate_original": "false",
    "trrack_graph": "ᯡ࠽䈌ʀ匦㢠⹰â䂬ƀᶷ琶䁋c<搔怳ଚ䠳亡䈤㉕⁖ì⺇⊨攩ᐠ▨戰㑹㧒妰囇⃹喧㝞Ţ∠桠ûு䁔䈠ॢȠந㺸ᢀ盅⻿夣•晀樠Ԃ牠Έƕ稦❳ሪὦ䰼⇑䅠Ȅ恨㙎⌯᪴䶹䈁ℓ琡㙐ᮂ⢨᢬थ⧓ࡢ㌰ሚ璋U䳘䠵欯Т縔絭Sㄭۡ۹ण䉔悊㘠传ϒ橷挣⌏ⱡ䕼庮打㈨ɼ榨慢䃂॑瑐繦┭ڼ㾬⎂厃焸牘榥ᄨ䦯ඦ嘔㗆溄⒀擒摖≦၀ⶃ㧄మޢᄴ䩍རへ∦䯧焤愷䍠Žƀ夅შ㤙༾′⪂㵩掳婊⢄  "
   },
   "source": [
    "Next, we'll introduce a new categorical variable named `Avalanche Season` into our dataset. This addition aims to classify each avalanche record into different parts of the avalanche season (Start, Middle, End) based on the month it occurred in.\n",
    "\n",
    "Create a new category `Avalanche Season` in the dataset and assign each record to `Start`, `Middle`, or `End` of the avalanche season based on its month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48aaa8-7017-43de-8bd8-190ed9fe4dbd",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Create New Variable:**\n",
    "    - Add a new column **`Avalanche Season`** to the DataFrame.\n",
    "2. **Assign Category:**\n",
    "    - Using the `month` from the `Date` column assign proper values to the new category.\n",
    "    - You should use the following ranges for assigning proper categories:\n",
    "        - `Start` of Season for _October_, _November_, _December_\n",
    "    \t- `Middle` of Season for _January_, _February_, _March_\n",
    "    \t- `End` of Season for _April, May_, _June_\n",
    "3. **Generate Dataframe:**\n",
    "    - Save the modified DataFrame with the new `Avalanche Season` category to `df_task_3a`.\n",
    "4. **Show Output:**\n",
    "    - Display the head of `df_task_3a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb77fc-69d5-4bfc-beff-ae54ad5c19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date time\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=df_task_2b[\"Month\"])\n",
    "\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('# of records')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "df_task_2b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e82a42-5f2f-44e1-b9a5-2e2d403b2ff3",
   "metadata": {},
   "source": [
    "## **Task 3b: Analyzing Top Avalanche Trigger by Season**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40bad3-436e-426c-a02f-f205597e90b5",
   "metadata": {},
   "source": [
    "Now we'll analyze which trigger is most prevalent for avalanches in each season phase (Start, Middle, End) using the `Avalanche Season` category created in Task 3a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de570e5-ab49-4e8f-b2b1-86b9a2701ece",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "1. **Context:**\n",
    "    - We have a facted bar chart. The `x` axis encodes the `Trigger` column in the data and the columns encode the newly added category `Avalanche Season`.\n",
    "2. **Analyze Trigger Data:**\n",
    "    - Observe the most common trigger for each season.\n",
    "    - You can hover on the bars to get the exact frequency.\n",
    "3. **Document Findings:**\n",
    "    - Note down the most common trigger for each season based on your interactive analysis in the markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967e8a0-84b9-4723-ab6f-c1660c7a46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_COLUMN = \"Avalanche Season\"\n",
    "\n",
    "selection = alt.selection_point(name=\"selector\", fields=[f\"{NEW_COLUMN}\"], bind=\"legend\")\n",
    "\n",
    "chart = alt.Chart(df_task_3a).mark_bar().encode(\n",
    "    x=\"Trigger:N\",\n",
    "    y=\"count():Q\",\n",
    "    color=alt.Color(f\"{NEW_COLUMN}:N\"),\n",
    "    column=f\"{NEW_COLUMN}:N\",\n",
    "    opacity=alt.condition(selection,alt.value(1), alt.value(0.3)),\n",
    "    tooltip=\"count()\"\n",
    ").transform_calculate(\n",
    "    selection_order=f\"if(selector && selector['{NEW_COLUMN}'], if((datum['{NEW_COLUMN}'] === selector['{NEW_COLUMN}'][0]),0,1),0)\"\n",
    ").add_params(selection)\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a13e2-c66f-45e6-ba89-2ad0c69afa58",
   "metadata": {},
   "source": [
    "**Task 3b Notes:**\n",
    "\n",
    "- Most common Trigger for `Start` of the season:  _name_ (_count_)\n",
    "- Most common Trigger for `Middle` of the season: _name_ (_count_)\n",
    "- Most common Trigger for `End` of the season: _name_ (_count_)"
   ]
  }
 ],
 "metadata": {
  "__CATEGORIES_META__": "{\"categories\":{\"Avlanche Season Phase\":{\"name\":\"Avlanche Season Phase\",\"options\":{\"_None\":{\"name\":\"None\"},\"Start\":{\"name\":\"Start\"},\"Middle\":{\"name\":\"Middle\"},\"End\":{\"name\":\"End\"}}}},\"activeCategoryName\":\"Avlanche Season Phase\"}",
  "__persist_keys_record": [
   "__GENERATED_DATAFRAMES__",
   "__persist_nb_uuid__",
   "trrack_graph",
   "show_aggregate_origin"
  ],
  "__persist_nb_uuid__": "b2bb15c7-81d2-4751-85ba-cc95c5781d33",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
